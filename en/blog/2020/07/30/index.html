<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta name="theme-color" content="#212b31">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/fonts.css">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
  <script type="text/javascript">
    window.transitionToPage = function(href) {
      document.querySelector('body').style.opacity = 0
      setTimeout(function() {
        window.location.href = href
        }, 500)
      }

    document.addEventListener('DOMContentLoaded', function(event) {
      document.querySelector('body').style.opacity = 1
    })
    const versions = {"fr": "/fr"}
  </script>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Rphad_en</title>
</head>
<body>
  <!-- Top navigation -->
  <div class="topnav">

    <!-- Left-aligned links (default) -->
    <a href="/" id="logo">
       R<span id="logo_fn">aphaël </span>
       D<span id="logo_ln">eknop </span>
    </a>

    <!--
    <div class="topnav-centered">
      <a href="#news" id="logo">
        R<span id="cas">aphaël</span>
        D<span id="sty">eknop</span>
      </a>
    </div>
    //-->

    <!-- Right-aligned links -->
    <div class="topnav-right">
      <a class="active">Blog</a>
      <a href="/en/about">About</a>
      <a href="/en/contact">Contact</a>
      <a href="/fr/blog/2020/07/30" class=icon title="Français">
        <svg width="24" height="24" id="langicon">
          <path d="M6.235 6.453a8 8 0 0 0 8.817 12.944c.115-.75-.137-1.47-.24-1.722-.23-.56-.988-1.517-2.253-2.844-.338-.355-.316-.628-.195-1.437l.013-.091c.082-.554.22-.882 2.085-1.178.948-.15 1.197.228 1.542.753l.116.172c.328.48.571.59.938.756.165.075.37.17.645.325.652.373.652.794.652 1.716v.105c0 .391-.038.735-.098 1.034a8.002 8.002 0 0 0-3.105-12.341c-.553.373-1.312.902-1.577 1.265-.135.185-.327 1.132-.95 1.21-.162.02-.381.006-.613-.009-.622-.04-1.472-.095-1.744.644-.173.468-.203 1.74.356 2.4.09.105.107.3.046.519-.08.287-.241.462-.292.498-.096-.056-.288-.279-.419-.43-.313-.365-.705-.82-1.211-.96-.184-.051-.386-.093-.583-.135-.549-.115-1.17-.246-1.315-.554-.106-.226-.105-.537-.105-.865 0-.417 0-.888-.204-1.345a1.276 1.276 0 0 0-.306-.43zM12 22C6.477 22 2 17.523 2 12S6.477 2 12 2s10 4.477 10 10-4.477 10-10 10zz">
          </path>
        </svg>
        &nbsp;fr
      </a>
    </div>

  </div>

  <article>
    <h1>A first look at Generative Adversarial Networks</h1>

      <h2>Introduction</h2>
        <p>
          In the last decade, machine learning and artificial intelligence have become increasingly ubiquitous. From autonomous vehicles to face recognition or even <a href="https://openai.com/projects/five/">beating world-class players in online games</a>, it is no wonder that this technology has an enormous potential and can be dangerously powerful. This document is aimed at the general public and intends to provide an intuitive -yet precise- understanding on what Generative Adversarial Networks are, what they are capable of, along with some further reflexions.
        </p>

      <h2>1. Discriminative and generative models</h2>
        <p>
          In machine learning, two main approaches can be followed. On the one hand, the <strong>discriminative</strong> method may be employed to tackle classifying problems such as assigning the correct label to an image, computing a highly probable output from a (previously unseen) complex input and much more. On the other hand, this model cannot <em>generate</em> similar data. An intuitive illustration of discriminative modelling in human behaviour is the capacity of distinguishing chinese characters while remaining unable to correctly draw one of them.
          <br>
          This is where the <strong>generative</strong> one differs. As its name suggests, a model using such an approach can generate data resembling what it has been fed. Here, as long as the input data is carefully selected, no label is required. Let's say that you want your model to generate a Shakespearean poem, it would be counterproductive to train it with some -unlabelled- Edgar Allan Poe or Oscar Wilde writings.
        </p>

      <h2>2. Generative Adversarial Networks</h2>
        <p>
          Although those two approaches were just presented separately, nothing is forbidding a network to follow both of them. This is where Generative Adversarial Networks come in. In a GAN, a generative network produces candidates while a discriminative one evaluates them. To put it simply, the goal of the generator is to fool the discriminator into thinking that the data provided is real.
          <br>
          This type of network, presented in 2014 by Ian Goodfellow and his collaborators [<a href="#goodfellow2014generative">Goo+14</a>], has been an enormous step in machine learning. To illustrate that statement, look at this series of generated human faces:
        </p>
        <figure>
            <img src="GAN_ghf.png" onmouseover="this.src='GAN_ghf_dt.png'" onmouseout="this.src='GAN_ghf.png'" width="75%">
            <figcaption style="text-align:center;">
              <span>Figure 1: </span>Progress of GANs on human face generation. Source: <a href="https://twitter.com/goodfellow_ian/status/1084973596236144640">Ian Goodfellow’s Twitter</a>
            </figcaption>
        </figure>
        <p>
          The evolution here is astonishing. Let's remember that before 2014, GANs simply didn't exist. There are a bunch of websites showing GAN-generated images of <a href="https://www.thispersondoesnotexist.com">people</a>, <a href="https://thisartworkdoesnotexist.com/">artworks</a>, <a href="https://thiscatdoesnotexist.com/">cats</a> or even <a href="https://thishorsedoesnotexist.com/">horses</a> produced by StyleGAN2 [<a href="#karras2019analyzing"> Kar+19</a>] for the curious out there. It is effortless -and an interesting exercise- to come up with potential applications and realise that there are plenty of them.
        </p>

      <h2>3. Going beyond GANs</h2>
        <p>
          There exists plenty of neural network types, generative adversarial networks are just one of them. The reason I presented them here is because I find them to be lying on the right spot between intuitive understanding and current state of the art in AI research. I haven't been technical here, as it is not the intention of this article.
          <br>
          Interestingly enough, GANs have their limitations. For example, it is impossible to tweak parameters on a generated human face picture in order to change, say, hair colour, face expression or nose shape. However, that is made possible by Adversarial Latent Autoencoders (ALAEs) [<a href="#pidhorskyi2020adversarial">PAD20</a>] which make use of <a href="https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d">latent spaces</a>.
          <br>
          <a href="#ALAE_kzf">Figure 2</a> contains screenshots from a video made by <em>Two Minute Papers</em> on ALAEs. Its creator, Karoly Zsolnai-Fehér, demonstrates the use of cursors on the right in order to tweak desired parameters. In the case of figures <a href="">2a</a> and 2b the cursor <em>mouth-open</em> is set from low to high. Notice how smile lines appear not only near the mouth but also around the eyes.
        </p>
        <figure id="ALAE_kzf">
          <figure>
              <img src="ALAE_kzf_1.png" onmouseover="this.src='ALAE_kzf_1_dt.png'" onmouseout="this.src='ALAE_kzf_1.png'" width="75%">
              <figcaption style="text-align:center;">
                <span>(a): </span>Lower <em>mouth-open</em> cursor
              </figcaption>
          </figure>
          <figure>
              <img src="ALAE_kzf_2.png" onmouseover="this.src='ALAE_kzf_2_dt.png'" onmouseout="this.src='ALAE_kzf_2.png'" width="75%">
              <figcaption style="text-align:center;">
                <span>(b): </span>Higher <em>mouth-open</em> cursor
              </figcaption>
          </figure>
            <figcaption style="text-align:center;"> Figure 2: Tweakable generated human faces produced by an ALAE. Screenshots from <a href="https://youtu.be/BQQxNa6U6X4?t=159">TMP’s video</a> on the subject.</figcaption>
        </figure>
      <h2>4. Discussion</h2>
        <p>
          Considering the available information and considering the actual progress made in AI and ML research, what could be achievable in the next decade ? In order to answer such a question, several ways of reasoning can be employed. In this case, I will adopt the following:
          <ol>
            <li>Acknowledge the current advancements in the topic you are studying</li>
            <li>Think about how (or if) they could be improved and polished individually</li>
            <li>Try to come up with new ways in which they could be combined to produce innovative results</li>
          </ol>
          This last point can be eased by merging different topics together. Let’s try with machine learning and cinema.
        </p>

        <p>
          Could creating a movie become a one-person job that would’t require actors or any equipment other than a computer ? We are not talking about animation here but rather in the order of giving a script as an input and obtaining a complete realistic movie as an output. We have seen that it is an unchallenging task to generate pictures of human faces and, as illustrated on figure 2, to make it smile. Could we create a latent space of facial expressions ? It seems like we can [<a href="#zhou2017photorealistic"> ZS17</a>]. Some more research and training and we are good to go. What about body movement ? Well, considering current papers, it looks promising [<a href="#aliaks2020order">Sia+20</a>; <a href="#Park:2019">Par+19a</a>]. This could go on with interior design [<a href="#mao2016squares">Mao+16</a>], landscapes [<a href="#park2019semantic">Par+19b</a>] (Nvidia set up a site where you can try it out yourself), text to image synthesis [<a href="#reed2016generative">Ree+16</a>], natural language processing [<a href="#brown2020language">Bro+20</a>] and way more.
        </p>

        <p>
          While each of all those cited works have their own flaws, I’d recommend looking at figure 1 again. That should give an indication on the room for improvement that exists for those applications. Combining the evoked technologies and letting them mature enough would create the ideal conditions in which our actorless movie could come to reality. The impacts of such an achievement could be huge. For example, if this way of creating films becomes the standard, it’s a whole industry that would collapse. This is of course still close to science-fiction but weren’t driverless cars science-fiction 10 or 20 years ago ?
        </p>
      <h2>5. Conclusion </h2>
        <p>
          AI research is making progress at a substantial pace and is shifting our society, the examples we have seen are only the tip of an iceberg whose size we ignore. The possibilities seem endless and as long as sufficient data and computing power are provided, the only limiting factor appears to be our imagination (and values). As with every powerful tool, we need to be vigilant with it, but the future looks fascinating.
        </p>
      <h2>References</h2>
      <table>
        <tr id="brown2020language">
          <td>[Bro+20]</td>
          <td>Tom <span style="font-variant: small-caps;">B. Brown</span> et al. <cite>Language Models are Few-Shot Learners.</cite> 2020. arXiv: <a href="https://arxiv.org/abs/2005.14165">2005.14165 [cs.CL]</a>.</td>
        </tr>
        <tr id="goodfellow2014generative">
          <td>[Goo+14]</td>
          <td>Ian <span style="font-variant: small-caps;">J. Goodfellow</span> et al. <cite>Generative Adversarial Networks.</cite> 2014. arXiv: <a href="https://arxiv.org/abs/1406.2661">1406.2661 [stat.ML]</a>.</td>
        </tr>
        <tr id="karras2019analyzing">
          <td>[Kar+19] </td>
          <td>Tero <span style="font-variant: small-caps;">Karras</span> et al. <cite>Analyzing and Improving the Image Quality of StyleGAN.</cite> 2019. arXiv: <a href="https://arxiv.org/abs/1912.04958">1912.04958 [cs.CV]</a>.</td> </tr>
        <tr id="mao2016squares">
          <td>[Mao+16]</td>
          <td>Xudong <span style="font-variant: small-caps;">Mao</span> et al. <cite>Least Squares Generative Adversarial Networks.</cite> 2016. arXiv: <a href="https://arxiv.org/abs/1611.04076">1611.04076 [cs.CV]</a>.</td> </tr>
        <tr id="pidhorskyi2020adversarial">
          <td>[PAD20] </td>
          <td>Stanislav <span style="font-variant: small-caps;">Pidhorskyi</span>, Donald <span style="font-variant: small-caps;">Adjeroh</span>, and Gianfranco <span style="font-variant: small-caps;">Doretto</span>. <cite>Adversarial Latent Autoencoders.</cite> [preprint]. 2020. arXiv: <a href="https://arxiv.org/abs/2004.04467">2004.04467 [cs.LG]</a>.</td> </tr>
        <tr id="Park:2019">
          <td>[Par+19a]</td>
          <td>Soohwan <span style="font-variant: small-caps;">Park</span> et al. <cite>Learning Predict-and-Simulate Policies From Unorganized Human Motion Data.</cite> 2019. URL: <a href="https://mrl.snu.ac.kr/publications/ProjectICC/ICC.pdf">mrl.snu.ac.kr/publications/ProjectICC/ICC.pdf</a>.</td> </tr>
        <tr id="park2019semantic">
          <td>[Par+19b] </td>
          <td>Taesung <span style="font-variant: small-caps;">Park</span> et al. <cite>Semantic Image Synthesis with Spatially-Adaptive Normalization.</cite> 2019. arXiv:<a href="https://arxiv.org/abs/1903.07291">1903.07291 [cs.CV]</a>.</tr>
        <tr id="reed2016generative">
          <td>[Ree+16]</td>
          <td>Scott <span style="font-variant: small-caps;">Reed</span> et al. <cite>Generative Adversarial Text to Image Synthesis.</cite> 2016. arXiv: <a href="https://arxiv.org/abs/1605.05396">1605.05396 [cs.NE]</a>.</td> </tr>
        <tr id="aliaks2020order">
          <td>[Sia+20]</td>
          <td> Aliaksandr <span style="font-variant: small-caps;">Siarohin</span> et al. <cite>First Order Motion Model for Image Animation.</cite> 2020. arXiv: <a href="https://arxiv.org/abs/2003.00196">2003.00196 [cs.CV]</a>.</td> </tr>
        <tr id="zhou2017photorealistic">
          <td>[ZS17]</td>
          <td>Yuqian <span style="font-variant: small-caps;">Zhou</span> and Bertram Emil <span style="font-variant: small-caps;">Shi</span>. <cite>Photorealistic Facial Expression Synthesis by the Conditional Difference Adversarial Autoencoder.</cite> 2017. arXiv: <a href="https://arxiv.org/abs/1708.09126">1708.09126 [cs.CV]</a>.</td> </tr>
      </table>
  </article>
</body>

<script>
  function LangChange(element) {
    window.location.href = versions[element.lang]
  }

</script>
</html>
