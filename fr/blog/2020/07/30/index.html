<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta name="theme-color" content="#212b31">
  <link rel="stylesheet" type="text/css" href="/css/variables.css">
  <link rel="stylesheet" type="text/css" href="/css/main.css">
  <link rel="stylesheet" type="text/css" href="/css/fonts.css">
  <link rel="stylesheet" type="text/css" href="/css/mobile.css">
  <link href="https://fonts.googleapis.com/css2?family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
  <script type="text/javascript">
    window.transitionToPage = function(href) {
      document.querySelector('body').style.opacity = 0
      setTimeout(function() {
        window.location.href = href
        }, 500)
      }

    document.addEventListener('DOMContentLoaded', function(event) {
      document.querySelector('body').style.opacity = 1
    })
    const versions = {"fr": "/fr"}
  </script>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Rphad</title>
</head>
<body>
  <div id="mySidepanel" class="sidepanel">
    <a href="../../.." class="active">Blog</a>
    <a href="../../../../about">À propos</a>
    <a href="../../../../contact" >Contact</a>
    <a href="/en/blog/2020/07/30" class=icon title="Switch to English" id="langlink">
      <svg width="24" height="24" id="langicon" />
      &nbsp;fr
    </a>
    <button onclick="changeTheme()" class="themebtn">
      <svg width="24" height="24" class="themeSvg" />
    </button>
  </div>

<!-- Top navigation -->
<div class="topnav">

  <!-- Left-aligned links (default) -->
  <a href="/fr" id="logo" title="Home">
     R<span id="logo_fn">aphaël </span>
     D<span id="logo_ln">eknop </span>
  </a>

  <!-- Right-aligned links -->
  <div class="topnav-right">
    <a href="../../.." class="active">Blog</a>
    <a href="../../../../about">À propos</a>
    <a href="../../../../contact" >Contact</a>
    <a href="/en/blog/2020/07/30" class=icon title="Switch to English" id="langlink">
      <svg width="24" height="24" id="langicon" />
      &nbsp;fr
    </a>

    <button onclick="changeTheme()" id="themebtn">
      <svg width="24" height="24" id="themeSvg" class="themeSvg"/>
    </button>

    <div class="container" onclick="MobileMenu(this)">
      <div class="bar1"></div>
      <div class="bar2"></div>
      <div class="bar3"></div>
    </div>
  </div>
</div>

  <article>
    <h1>Un premier coup d'œil aux Réseaux Antagonistes Génératifs</h1>

      <h2>Introduction</h2>
        <p>
          Dans la dernière décennie, l'apprentissage automatique (<em>Machine learning</em>) et l'intelligence artificielle ont pris de plus en plus de place dans nos vies. Des véhicules autonomes à la reconnaissance faciale, en passant par la <a href="https://openai.com/projects/five/" target="_blank">victoire contre des joueurs de classe mondiale dans des jeux en ligne</a>, il n'est pas étonnant que cette technologie possède un énorme potentiel et puisse être dangereusement puissante. Ce document vise le grand public et a pour intention de fournir une compréhension intuitive -et toutefois précise- de ce que sont des Réseaux Antagonistes Génératifs (GANs) et de ce dont ils sont capables, accompagnée de réflexions plus élaborées. <strong>N.B.</strong> Bien que leur acronyme soit <em>RAG</em>, nous utiliserons l'abbréviation anglaise <em>GAN</em> pour <em>General Adversarial Network</em>, cette dernière étant bien plus courante.
        </p>

      <h2>1. Les modèles discriminatifs et génératifs</h2>
        <p>


          Dans le machine learning, deux méthodes principales peuvent être suivies. D'un côté, la méthode <strong>discriminative</strong> peut s'atteler à des problèmes\footnote{dans le sens épistémologique du terme} de classification tels qu'assigner l'étiquette (<em>label</em>) correcte à une image, calculer une valeur de sortie (<em>output</em>) hautement probable à partir de données d'entrée (<em>input</em>) complexes et jamais vues auparavant, et bien plus encore. De l'autre, ce modèle ne peut pas <em>générer</em> des données similaires. Une illustration intuitive d'un modèle discriminatif dans le comportement humain est la capacité à distinguer des signes chinois tout en étant incapable d'en écrire un seul correctement.
          <br>
          C'est là que la méthode <strong>générative</strong> diffère. Comme son nom le suggère, un modèle utilisant une telle approche peut générer des données qui ressemblent à ce qui lui a été fourni. Ici, pour autant que les données d'entrée soient sélectionées consciencieusement, aucun <em>label</em> n'est nécessaire. Supposons que vous vouliez générer un poème Baudelairien, il serait contreproductif de l'entraîner avec des écrits -non labellisés- venant de Victor Hugo ou Paul Verlaine.
        </p>

      <h2>2. Les Réseaux Antagonistes Génératifs</h2>
        <p>
          Bien que ces deux approches viennent d'être présentées séparément, rien n'empêche un réseau de les suivre toutes deux. C'est ici que les Réseaux Antagonistes Généraux entrent en jeu. Dans un GAN, un réseau génératif produits des candidats qu'un réseau discriminatif évaluera. En d'autres mots, l'objectif du générateur est de faire croire au discriminateur que les données fournies sont réelles.
          <br>
          Ce type de réseau, présenté en 2014 par Ian Goodfellow et ses collaborateurs [<a href="#goodfellow2014generative">Goo+14</a>], fut une étape énorme en machine learning. Pour illustrer ce propos, regardez cette suite de visages humains générés :
        </p>
        <figure id="GAN_ghf">
            <img src="GAN_ghf.png" onmouseover="this.src='GAN_ghf_dt.png'" onmouseout="this.src='GAN_ghf.png'" width="75%">
            <figcaption style="text-align:center;">
              <span>Figure 1: </span>Progrès des GANs en génération de visages humains. Source: <a href="https://twitter.com/goodfellow_ian/status/1084973596236144640" target="_blank">Ian Goodfellow sur Twitter</a>
            </figcaption>
        </figure>
        <p>
          L'évolution ici est stupéfiante. Souvenons-nous qu'avant 2014, les GANs n'existaient tout simplement pas. Il y a une poignée de sites web qui montrent des images GAN-générées de <a href="https://www.thispersondoesnotexist.com" target="_blank">personnes</a>, d'<a href="https://thisartworkdoesnotexist.com/" target="_blank">œuvres d'art</a>, de <a href="https://thiscatdoesnotexist.com/" target="_blank">chats</a> or même <a href="https://thishorsedoesnotexist.com/" target="_blank">chevaux</a> produites par StyleGAN2 [<a href="#karras2019analyzing"> Kar+19</a>] pour les curieux d'entre vous. C'est sans effort -et un exercice intéressant- que l'on peut trouver des applications potentielles et réaliser qu'il en existe une pléthore.
        </p>

      <h2>3. Au-delà des GANs</h2>
        <p>
          Il existe plein de types de réseaux neuronaux, les réseaux antagonistes génératifs sont juste l'un d'entre eux. La raison pour laquelle je les ai présentés ici est que je les trouve parfaitement placés entre une compréhension intuitive et l'état de la recherche en IA. Je n'ai pas été fort technique ici, car ce n'est pas l'intention de cet article.
          <br>
          De manière assez intéressante, les GANs possèdent leurs limitations. Par exemple, il est impossible de configurer les paramètres d'une image de visage humain en vue de changer, disons, la couleur des cheveux, l'expression du visage ou la forme du nez. Par contre, cela est rendu possible par des Auto-encodeurs Latents Antagonistes (ALAEs, pour <em>Adversarial Latent Autoencoders</em>) [<a href="#pidhorskyi2020adversarial">PAD20</a>] qui font usage des <a href="https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d" target="_blank">espaces latents</a>.
          <br>
          La <a href="#ALAE_kzf">Figure 2</a> contient des screenshots d'une vidéo produite par <em>Two Minute Papers</em> sur les ALAEs. Son créateur, Karoly Zsolnai-Fehér, démontre l'usage des curseurs de droite afin de changer les paramètres désirés. Dans le cas des figures <a href="ALAE_kzf_1">2a</a> et <a href="ALAE_kzf_2">2b</a>, le curseur <em>mouth-open</em> est changé vers une valeur plus haute. Nous remarquons que les changements des plis du visages sont non seulement présents près de la bouche, mais aussi autour des yeux.
        </p>
        <figure id="ALAE_kzf">
          <figure id="ALAE_kzf_1">
              <img src="ALAE_kzf_1.png" onmouseover="this.src='ALAE_kzf_1_dt.png'" onmouseout="this.src='ALAE_kzf_1.png'" width="75%">
              <figcaption style="text-align:center;">
                <span>(a): </span>Curseur <em>mouth-open</em> bas
              </figcaption>
          </figure>
          <figure id="ALAE_kzf_2">
              <img src="ALAE_kzf_2.png" onmouseover="this.src='ALAE_kzf_2_dt.png'" onmouseout="this.src='ALAE_kzf_2.png'" width="75%">
              <figcaption style="text-align:center;">
                <span>(b): </span>Curseur <em>mouth-open</em> haut
              </figcaption>
          </figure>
            <figcaption style="text-align:center;"> Figure 2: Visages générés paramétrisables produits par un ALAE. Screenshots d'une <a href="https://youtu.be/BQQxNa6U6X4?t=159" target="_blank">vidéo de TMP</a> sur le sujet.</figcaption>
        </figure>
      <h2>4. Discussion</h2>
        <p>
          En considérant les informations disponibles et le progrès actuel fait dans la recherche en IA et machine learning, que serait-il réalisable dans la prochaine décennie ? Afin de répondre à une telle question, plusieurs manières de raisonner peuvent être employées. Dans ce cas-ci, j'utiliserai la suivante :
          <ol>
            <li>Prendre connaissance des avancements actuels dans le domaines que l'on étudie</li>
            <li>Réfléchir à comment (et <em>si</em>) il peuvent être améliorés ou perfectionnés individuellement</li>
            <li>Trouver des nouvelles façons de les combiner pour produire des résultats novateurs</li>
          </ol>
          Ce dernier point peut être facilité en fusionnant différents sujets ensemble. Essayons avec le machine learning et le cinéma.
        </p>

        <p>
          <strong>Créer un film pourrait-il devenir un travail fait par une seule personne, qui ne requiérerait aucun acteur ni aucun équipement autre qu'un ordinateur ?</strong> Nous ne parlons pas d'animation ici mais plutôt de donner un script comme donnée d'entrée à un programme et obtenir un film complet en sortie. Nous avons vu que c'est une tâche sans effort que de générer des images de visages humains et, comme illustré sur la <a href="#ALAE_kzf">figure 2</a>, de les faire sourire. Pourrions-nous créer un espace latent d'expressions faciales ? Il semblerait que nous puissions le faire [<a href="#zhou2017photorealistic"> ZS17</a>]. Un peu plus de recherche et d'entraînement et ce sera bon. Qu'en est-il des mouvements corporels ? Eh bien, en considérant les papiers actuels, cela a l'air prometteur [<a href="#aliaks2020order">Sia+20</a>; <a href="#Park:2019">Par+19a</a>]. Cela pourrait continuer avec du design d'intérieur [<a href="#mao2016squares">Mao+16</a>], des paysages [<a href="#park2019semantic">Par+19b</a>] (Nvidia a mis en ligne un site où l'on peut <a href="http://nvidia-research-mingyuliu.com/gaugan"> expérimenter avec leur GAN</a>), la synthèse texte-vers-image [<a href="#reed2016generative">Ree+16</a>], le traitement du langage naturel [<a href="#brown2020language">Bro+20</a>] et bien plus encore.
        </p>

        <p>
          Bien que chacun des articles cités aient leurs failles, je conseillerais de regarder la <a href="#GAN_ghf">figure 1</a> à nouveau. Cela devrait donner un indication de la place à l'amélioration qu'il existe pour ces applications. Combiner les technologies évoquées et les laisser suffisamment mûrir créerait les conditions idéales dans lesquelles notre film sans acteur pourrait devenir réalité. Les impacts d'une telle réalisation pourraient être immenses. Par exemple, si cette façon de créer des films devient la norme, c'est toute une industrie qui s'effondrerait. Cela reste bien sûr encore proche de la science-fiction mais les voitures autonomes n'étaient-elles pas de la science-fiction il y a 10 ou 20 ans?
        </p>
      <h2>5. Conclusion </h2>
        <p>
          La recherche sur l'IA progresse à un rythme considérable et transforme notre société, les exemples que nous avons vus ne sont que la partie émergée d'un iceberg dont nous ignorons la taille. Les possibilités semblent infinies et, tant que des données et une puissance de calcul suffisantes sont fournies, le seul facteur limitant semble être notre imagination (et nos valeurs). Comme pour tout outil puissant, nous devons être vigilants en l'employant, mais l'avenir semble fascinant.
        </p>
      <h2>Références</h2>
      <table>
        <tr id="brown2020language">
          <td>[Bro+20]</td>
          <td>Tom <span style="font-variant: small-caps;">B. Brown</span> et al. <cite>Language Models are Few-Shot Learners.</cite> 2020. arXiv: <a href="https://arxiv.org/abs/2005.14165" target="_blank">2005.14165 [cs.CL]</a>.</td>
        </tr>
        <tr id="goodfellow2014generative">
          <td>[Goo+14]</td>
          <td>Ian <span style="font-variant: small-caps;">J. Goodfellow</span> et al. <cite>Generative Adversarial Networks.</cite> 2014. arXiv: <a href="https://arxiv.org/abs/1406.2661" target="_blank">1406.2661 [stat.ML]</a>.</td>
        </tr>
        <tr id="karras2019analyzing">
          <td>[Kar+19] </td>
          <td>Tero <span style="font-variant: small-caps;">Karras</span> et al. <cite>Analyzing and Improving the Image Quality of StyleGAN.</cite> 2019. arXiv: <a href="https://arxiv.org/abs/1912.04958" target="_blank">1912.04958 [cs.CV]</a>.</td> </tr>
        <tr id="mao2016squares">
          <td>[Mao+16]</td>
          <td>Xudong <span style="font-variant: small-caps;">Mao</span> et al. <cite>Least Squares Generative Adversarial Networks.</cite> 2016. arXiv: <a href="https://arxiv.org/abs/1611.04076" target="_blank">1611.04076 [cs.CV]</a>.</td> </tr>
        <tr id="pidhorskyi2020adversarial">
          <td>[PAD20] </td>
          <td>Stanislav <span style="font-variant: small-caps;">Pidhorskyi</span>, Donald <span style="font-variant: small-caps;">Adjeroh</span>, and Gianfranco <span style="font-variant: small-caps;">Doretto</span>. <cite>Adversarial Latent Autoencoders.</cite> [preprint]. 2020. arXiv: <a href="https://arxiv.org/abs/2004.04467" target="_blank">2004.04467 [cs.LG]</a>.</td> </tr>
        <tr id="Park:2019">
          <td>[Par+19a]</td>
          <td>Soohwan <span style="font-variant: small-caps;">Park</span> et al. <cite>Learning Predict-and-Simulate Policies From Unorganized Human Motion Data.</cite> 2019. URL: <a href="https://mrl.snu.ac.kr/publications/ProjectICC/ICC.pdf" target="_blank">mrl.snu.ac.kr/publications/ProjectICC/ICC.pdf</a>.</td> </tr>
        <tr id="park2019semantic">
          <td>[Par+19b] </td>
          <td>Taesung <span style="font-variant: small-caps;">Park</span> et al. <cite>Semantic Image Synthesis with Spatially-Adaptive Normalization.</cite> 2019. arXiv:<a href="https://arxiv.org/abs/1903.07291" target="_blank">1903.07291 [cs.CV]</a>.</tr>
        <tr id="reed2016generative">
          <td>[Ree+16]</td>
          <td>Scott <span style="font-variant: small-caps;">Reed</span> et al. <cite>Generative Adversarial Text to Image Synthesis.</cite> 2016. arXiv: <a href="https://arxiv.org/abs/1605.05396" target="_blank">1605.05396 [cs.NE]</a>.</td> </tr>
        <tr id="aliaks2020order">
          <td>[Sia+20]</td>
          <td> Aliaksandr <span style="font-variant: small-caps;">Siarohin</span> et al. <cite>First Order Motion Model for Image Animation.</cite> 2020. arXiv: <a href="https://arxiv.org/abs/2003.00196" target="_blank">2003.00196 [cs.CV]</a>.</td> </tr>
        <tr id="zhou2017photorealistic">
          <td>[ZS17]</td>
          <td>Yuqian <span style="font-variant: small-caps;">Zhou</span> and Bertram Emil <span style="font-variant: small-caps;">Shi</span>. <cite>Photorealistic Facial Expression Synthesis by the Conditional Difference Adversarial Autoencoder.</cite> 2017. arXiv: <a href="https://arxiv.org/abs/1708.09126" target="_blank">1708.09126 [cs.CV]</a>.</td> </tr>
      </table>
  </article>
</body>

<script type="text/javascript" src="/js/mobile.js"></script>
<script type="text/javascript" src="/js/main.js"></script>
<script type="text/javascript" src="/js/cleantheme.js"></script>
<script>
  function LangChange(element) {
    window.location.href = versions[element.lang]
  }
</script>
</html>
